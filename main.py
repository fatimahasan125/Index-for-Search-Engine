# -*- coding: utf-8 -*-
"""L174020_IR_Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RHB9Xv01qqUk8QDJ1ULL0cnoQTqZLJLE
"""

import os
from bs4 import BeautifulSoup
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import nltk
from itertools import groupby
from math import sqrt

nltk.download('stopwords')


def tokenizer(directory, id):
    files = os.listdir(directory);
    token_information = []
    for file in files:
        process = False
        print(file)

        try:
            contents = open(directory + "/" + file, "r").read()
            contents = contents[contents.index('<!DOCTYPE'):]
            process = True
        except ValueError or UnicodeDecodeError:
            try:
                contents = contents[contents.index('<html'):]  # some files dont begin with doctype
                process = True
            except:
                pass

        if process == True:
            id = id + 1
            soup = BeautifulSoup(contents, features="html.parser")
            for useless_tag in soup(["script", "style"]):
                useless_tag.extract()  # remove the script and style tags because they dont have any text content

            text = soup.get_text().splitlines()
            lines = (line.strip() for line in text)
            lines = (heading.strip() for line in lines for heading in line.split("  "))
            text = '\n'.join(line for line in lines if line)

            text = text.lower()

            length = 0
            for word in text.split():
                length = length + 1

            # removing unecessary punctuation
            punc_and_digits = '''!()[`]{};:'"\, <>.=/?@’#$“%+-^&*_~0123456789'''
            for letter in text:
                if letter not in " " and letter in punc_and_digits:
                    text = text.replace(letter, "")

                    # removing 1 letter words
            tokens = []
            position = 0
            for token in text.split():
                if len(token) > 1:
                    tokens.append([token, id, position])
                position = position + 1

            # stopwording
            sw = stopwords.words("english")
            for stopword in sw:
                for sublist in tokens:
                    if stopword in sublist:
                        tokens.remove(sublist)

            # stemming using porter stemmer
            ps = PorterStemmer()

            words = []
            for token in tokens:
                token_stemmed = ps.stem(token[0])
                words.append(token_stemmed)
                token_information.append([token_stemmed, token[1], token[2]])

            words_freq = [len(list(group)) for key, group in groupby(words)]
            magnitude = sqrt(sum(map(lambda x: x * x, words_freq)))

            docinfo = open("docInfo.txt", "a")
            docinfo.write(str(id) + "," + os.path.basename(os.path.normpath(directory)) + "/" + file + "," + str(length) + "," + str(magnitude) + "\n")

    return token_information

def create_index(token_information):
    index = {}
    for token in token_information:
        if token[0] not in index:  # for the first time we are encountering the token, ever
            index[token[0]] = [1, token[1], 1, token[2]]  # token[0] is the word, token[1] is the doc id,
            # token[2] is the position of the word in the document
        else:  # if the token exists
            df = index[token[0]][0]
            doc_index = 2
            docs = []
            for i in range(df):
                docs.append(index[token[0]][doc_index - 1])
                doc_index = doc_index + index[token[0]][doc_index] + 2

            if token[1] not in docs:  # first occurrence of token in current document
                index[token[0]][0] = df + 1  # incrementing document frequency
                index[token[0]].extend([token[1], 1, token[2]])  # term frequency set to 1

            else:
                doc_index = 2
                for i in range(df):
                    if index[token[0]][doc_index - 1] == token[1]:
                        break
                    doc_index = doc_index + index[token[0]][doc_index] + 2

                # now doc_index is pointing to the term frequency in current document

                index[token[0]].insert(doc_index + index[token[0]][doc_index] + 1, token[2])
                index[token[0]][doc_index] = index[token[0]][doc_index] + 1  # incrementing term frequency

    # delta encoding the index
    for token in index:
        df = index[token][0]
        doc = index[token][1]
        doc_freq_index = 2

        for i in range(df):
            if i != df - 1:
                new_doc_index = doc_freq_index + index[token][doc_freq_index] + 1  # updating the document number
                new_doc = index[token][new_doc_index]
                index[token][new_doc_index] = index[token][new_doc_index] - doc
                doc = new_doc
            if index[token][doc_freq_index] > 1:  # updating the positions
                term_pos = index[token][doc_freq_index + 1]
                for j in range(doc_freq_index + 2, doc_freq_index + index[token][doc_freq_index] + 1):
                    new_pos = index[token][j]
                    index[token][j] = index[token][j] - term_pos
                    term_pos = new_pos

            doc_freq_index = doc_freq_index + index[token][doc_freq_index] + 2

    return index


def write_to_file(file1, file2, index):
    offset = 0
    i = 0
    for token in index:
        try:
            file1.write(token[0] + "," + str(offset) + ",")
            file2.write(str(token[1:])[2:-3])
            no_of_bytes = file2.tell() - offset
            file1.write(str(no_of_bytes) + "\n")
            file2.write(" ")
            offset = file2.tell()
        except:
            pass

    file1.close()
    file2.close()


def merge_indexes(index1_terms_path, index1_postings_path, index2_terms_path, index2_postings_path,
                  index3_terms_path, index3_postings_path):
    index1_terms = open(index1_terms_path, "r")
    index1_postings = open(index1_postings_path, "r")

    index2_terms = open(index2_terms_path, "r")
    index2_postings = open(index2_postings_path, "r")

    index3_terms = open(index3_terms_path, "r")
    index3_postings = open(index3_postings_path, "r")

    merged_index_terms = open("inverted_index_terms.txt", "w")
    merged_index_postings = open("inverted_index_postings.txt", "w")
    merged_postings_offset = 0
    index_ids = [1, 2, 3]

    while True:
        if 1 in index_ids:
            index1_terms_line = index1_terms.readline()
        if 1 in index_ids and index1_terms_line:
            index1_terms_line = index1_terms_line[0:-1].split(",")
            index1_postings.seek(int(index1_terms_line[1]))
            index1_postings_line = index1_postings.readline(int(index1_terms_line[2]) + 1)
            index1_postings_line = [int(s) for s in index1_postings_line[0:-1].split(", ") if s != '']

        if 2 in index_ids:
            index2_terms_line = index2_terms.readline()
        if 2 in index_ids and index2_terms_line:
            index2_terms_line = index2_terms_line[0:-1].split(",")
            index2_postings.seek(int(index2_terms_line[1]))
            index2_postings_line = index2_postings.readline(int(index2_terms_line[2]) + 1)
            index2_postings_line = [int(s) for s in index2_postings_line[0:-1].split(", ") if s != '']

        if 3 in index_ids:
            index3_terms_line = index3_terms.readline()
        if 3 in index_ids and index3_terms_line:
            index3_terms_line = index3_terms_line[0:-1].split(",")
            index3_postings.seek(int(index3_terms_line[1]))
            index3_postings_line = index3_postings.readline(int(index3_terms_line[2]) + 1)
            index3_postings_line = [int(s) for s in index3_postings_line[0:-1].split(", ") if s != '']

        words_list = []
        if index1_terms_line:
            words_list.append(index1_terms_line[0])

        if index2_terms_line:
            words_list.append(index2_terms_line[0])

        if index3_terms_line:
            words_list.append(index3_terms_line[0])

        if len(words_list) == 0:
            break;
        word = min(w for w in words_list if w is not None)

        index_ids = []
        if index1_terms_line and word == index1_terms_line[0]:
            index_ids.append(1)
        if index2_terms_line and word == index2_terms_line[0]:
            index_ids.append(2)
        if index3_terms_line and word == index3_terms_line[0]:
            index_ids.append(3)

        df = 0
        if 1 in index_ids and index1_postings_line:
            df = df + int(index1_postings_line[0])

        if 2 in index_ids and index2_postings_line:
            df = df + int(index2_postings_line[0])

        if 3 in index_ids and index3_postings_line:
            df = df + int(index3_postings_line[0])

        merged_postings = [df]
        sum = 0
        if 1 in index_ids and index1_postings_line:
            merged_postings.extend(index1_postings_line[1:])
            iter = 1
            for i in range(0, int(index1_postings_line[0])):
                sum = sum + index1_postings_line[iter]
                iter = iter + index1_postings_line[iter + 1] + 2

        if 2 in index_ids and index2_postings_line:
            index2_postings_line[1] = index2_postings_line[1] - sum  # delta encoding for the first doc in posting list
            merged_postings.extend(index2_postings_line[1:])
            iter = 1
            for i in range(0, int(index2_postings_line[0])):
                sum = sum + index2_postings_line[iter]
                iter = iter + index2_postings_line[iter + 1] + 2

        if 3 in index_ids and index3_postings_line:
            index3_postings_line[1] = index3_postings_line[1] - sum
            merged_postings.extend(index3_postings_line[1:])

        merged_index_terms.write(word + "," + str(merged_postings_offset) + ",")
        merged_index_postings.write(str(merged_postings)[1:-1])
        no_of_bytes = merged_index_postings.tell() - merged_postings_offset
        merged_index_terms.write(str(no_of_bytes) + "\n")
        merged_index_postings.write(" ")
        merged_postings_offset = merged_index_postings.tell()

    merged_index_terms.close()
    merged_index_postings.close()


def retrieve_documents(query):
    sw = stopwords.words("english")
    ps = PorterStemmer()

    for term in query.split():
        if len(term) > 1:
            punc_and_digits = '''!()[`]{};:'"\, <>.=/?@’#$“%+-^&*_~0123456789'''
            for letter in term:
                if letter not in " " and letter in punc_and_digits:
                    term = term.replace(letter, "")
            term = term.lower()
            for stopword in sw:
                if term == stopword:
                    query = query.replace(term, "")

            term = ps.stem(term)

        else:
            query = query.replace(term, "")

    index = open("inverted_index_terms.txt", "r")
    postings = open("inverted_index_postings.txt", "r")
    list_of_docs = []

    index_line = index.readline()
    while index_line:
        index_line = index_line[0:-1].split(",")
        for term in query.split():
            if term == index_line[0]:
                postings.seek(int(index_line[1]))
                posting = postings.read(int(index_line[2])).split(", ")
                df = int(posting[0])
                documents = []
                iter = 1
                sum = 0
                for i in range(df):
                    documents.append(int(posting[iter]) + sum)
                    sum = sum + int(posting[iter])
                    iter = iter + int(posting[iter + 1]) + 2
                list_of_docs.append(documents)
                query = query.replace(term, "")

        index_line = index.readline()
        if not query.split():  # the entire query is processed
            break

    if not list_of_docs:
        print("No results found")

    else:
        doc_names = []
        res = list(set.intersection(*map(set, list_of_docs)))
        docs_info = open("docInfo.txt", "r")
        for id in res:
            doc = docs_info.readline().split(',')
            while int(doc[0]) != id:
                doc = docs_info.readline().split(',')
            doc_names.append(doc[1])

        doc_names = sorted(doc_names)
        for name in doc_names:
            print(name)


# TASK 1 (Preprocessing)
id = 0
tokens1 = tokenizer("corpus/1", id)
id = tokens1[-1][1]  # id of the last document
tokens2 = tokenizer("corpus/2", id)
id = tokens2[-1][1]
tokens3 = tokenizer("corpus/3", id)

# TASK 2 STEP 1 (Creating inverted index)
index1 = sorted(create_index(tokens1).items())
index2 = sorted(create_index(tokens2).items())
index3 = sorted(create_index(tokens3).items())

# TASK 2 STEP 2 (Writing index to file)
file1 = open("index_1_terms.txt", "w")
file2 = open("index_1_postings.txt", "w")
write_to_file(file1, file2, index1)

file1 = open("index_2_terms.txt", "w")
file2 = open("index_2_postings.txt", "w")
write_to_file(file1, file2, index2)

file1 = open("index_3_terms.txt", "w")
file2 = open("index_3_postings.txt", "w")
write_to_file(file1, file2, index3)

# TASK 3 (Merging Indexes)
merge_indexes("index_1_terms.txt", "index_1_postings.txt", "index_2_terms.txt", "index_2_postings.txt",
              "index_3_terms.txt", "index_3_postings.txt")

# TASK 4 is done while tokenizing the document

# TASK 5 (Reading Inverted Index)
q = input("Enter Query: ")
while( q != "exit" ):
    retrieve_documents(q)
    q = input("Enter Query: ")



